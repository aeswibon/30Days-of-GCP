## Contents
1. [AI Platform (Qwik Start)](#AI%20Platform(Qwik%20Start))
2. [Dataflow (Qwik Start)](#Dataflow%20(Qwik%20Start))
3. [Dataproc (Qwik Start)](#Dataproc%20(Qwik%20Start))
4. [Cloud Natural Language API(Qwik Start)](#Cloud%20Natural%20Language%20API%20(Qwik%20Start))
5. [Challenge Lab](#Challenge%20Lab)
---
#### AI Platform (Qwik Start)
- Bucket command
	```bash
	# export bucket name as env variable
	# export BUCKET_NAME=<your-unique-name>
	export BUCKET_NAME=gse_cloud
	#
	# create a bucket
	# -c -> class ie standard/nearline/..
	# -l -> location
	# gsutil mb gs://$BUCKET_NAME/
	gsutil mb -c standard -l us-east1 gs://$BUCKET_NAME/
	#
	# download the file
	wget --output-document ada.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Ada_Lovelace_portrait.jpg/800px-Ada_Lovelace_portrait.jpg
	#
	# copy the file to the bucket
	# gsutil cp ada.jpg gs://YOUR-BUCKET-NAME
	gsutil cp ada.jpg gs://gse_cloud
	#
	# remove the image
	rm ada.jpg
	#
	# download the object from the bucket
	# gsutil cp -r gs://YOUR-BUCKET-NAME/<file_path>
	gsutil cp -r gs://gse_cloud/ada.jpg
	#
	# copy an object to a folder in the bucket
	# gsutil cp gs://YOUR-BUCKET-NAME/<file_path> gs://YOUR-BUCKET-NAME/<folder_path>/
	gsutil cp gs://gse_cloud/ada.jpg gs://gse_cloud/image-folder/
	#
	# list contents of the bucket or folder
	# gsutil ls gs://YOUR-BUCKET-NAME
	gsutil ls gs://gse_cloud
	#
	# list details of an object
	# gsutil ls -l gs://YOUR-BUCKET-NAME/<file_path>
	gsutil ls -l gs://gse_cloud/ada.jpg
	#
	# make your object publicly accessible
	# gsutil acl ch -u AllUsers:R gs://YOUR-BUCKET-NAME/<file_path>
	gsutil acl ch -u AllUsers:R gs://gse_cloud/ada.jpg
	#
	# remove public access
	# gsutil acl ch -d AllUsers gs://YOUR-BUCKET-NAME/<file_path>
	gsutil acl ch -d AllUsers gs://gse_cloud/ada.jpg
	#
	# delete objects
	# gsutil rm gs://YOUR-BUCKET-NAME/<file_path>
	gsutil rm gs://gse_cloud/ada.jpg
	```
---
#### Dataflow (Qwik Start)
- BigQuery dataset
	```bash
	# create a dataset
	# bq mk <dataset_name>
	bq mk taxirides
	#
	# create a BigQuery table
	bq mk --time_partitioning_field=timestamp --schema=ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer -t taxirides.realtime
	#
	# run the pipeline
	# gcloud dataflow jobs run <job_name> --gcs-location gs://dataflow-templates-us-central1/latest/<dataFlow_template> --region us-central1 --staging-location gs://gse_cloud/temp --parameters inputTopic=<**Input Pub/Sub topic**>,outputTableSpec=<name_of_the_table_created>
	gcloud dataflow jobs run iotflow --gcs-location gs://dataflow-templates-us-central1/latest/PubSub_to_BigQuery --region us-central1 --staging-location gs://gse_cloud/temp --parameters inputTopic=projects/pubsub-public-data/topics/taxirides-realtime,outputTableSpec=<myprojectid>:taxirides.realtime
	```
---
#### Dataproc (Qwik Start)
- Create a cluster
	```bash
	# config the region
	gcloud config set dataproc/region us-central1
	#
	# create a cluster with default cloud dataproc settings
	# gcloud dataproc clusters create <name> --worker-boot-disk-size=500
	gcloud dataproc clusters create example-cluster --worker-boot-disk-size=500
	#
	# submit a job
	# gcloud dataproc jobs submit spark (--class=MAIN_CLASS | --jar=MAIN_JAR) (--cluster=CLUSTER | --cluster-labels=[KEY=VALUE,…]) [--archives=[ARCHIVE,…]] [--async] [--bucket=BUCKET] [--driver-log-levels=[PACKAGE=LEVEL,…]] [--files=[FILE,…]] [--jars=[JAR,…]] [--labels=[KEY=VALUE,…]] [--max-failures-per-hour=MAX_FAILURES_PER_HOUR] [--max-failures-total=MAX_FAILURES_TOTAL] [--properties=[PROPERTY=VALUE,…]] [--region=REGION] [GCLOUD_WIDE_FLAG …] [-- JOB_ARGS …]
	gcloud dataproc jobs submit spark --cluster example-cluster --class=org.apache.spark.examples.SparkPi --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar -- 1000
	```
---
####  Cloud Natural Language API (Qwik Start)
- Create an API Key
	```bash
	# export PROJECT_ID as env variable
	export GOOGLE_CLOUD_PROJECT=$(gcloud config get-value core/project)
	#
	# create a new service account to access Natural Language API
	# gcloud iam service-accounts create NAME [--description=DESCRIPTION] [--display-name=DISPLAY_NAME] [GCLOUD_WIDE_FLAG …] 
	gcloud iam service-accounts create my-natlang-sa
	```
---
#### Challenge Lab
- Step 1:
	```bash
	# start
	# create a dataset
	bq mk lab
	#
	# export YOUR_PROJECT env variable
	export YOUR_PROJECT=<project_id>
	#
	# create a bucket
	gsutil mb gs://$YOUR_PROJECT/
	# create a job
	gcloud dataflow jobs run Job --gcs-location=gs://dataflow-templates-us-central1/latest/GCS_Text_to_BigQuery --region=us-central1 --staging-location=gs://$YOUR_PROJECT/temp --parameters javascriptTextTransformGcsPath=gs://cloud-training/gsp323/lab.js,JSONPath=gs://cloud-training/gsp323/lab.schema,javascriptTextTransformFunctionName=transform,outputTable=$YOUR_PROJECT:lab.customers,inputFilePattern=gs://cloud-training/gsp323/lab.csv,bigQueryLoadingTemporaryDirectory=gs://$YOUR_PROJECT/bigquery_temp
	# end
	
	```
- Step 2(a):
	```bash
	# start
	# set the region
	gcloud config set dataproc/region us-central1
	#
	# create a cluster
	gcloud dataproc clusters create gse --worker-boot-disk-size=500
	#
	# end
	
	```
-  Step 2(b): open anyone cluster and click on ssh command and then type this command
	```bash
	hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
	```
- Step 2(c): open job tab in same dataproc and fill the table according to the data given.
- Step 3: 
	- Open Dataprep. Import the file and and then line by line add recipe. (Note: after every recipe add)
		```txt
		filter type: custom rowType: single row: column10 == 'SUCCESS' action: Keep
		```
		```txt
		filter type: contains col: column9 contains: /(^0$|^0\.0$)/ action: Delete
		```
		```txt
		rename type: manual mapping: [column2,'runid'], [column3,'userid'], [column4,'labid'], [column5,'lab_title'], [column6,'start'], [column7,'end'], [column8,'time'], [column9,'score'], [column10,'state']
		```
- Step 4:
	```bash
	# start
	# export YOUR_PROJECT env variable
	export YOUR_PROJECT=<project_id>
	#
	# create a bucket
	gsutil mb gs://$YOUR_PROJECT-marking/
	#
	# end
	
	```
	- Task 1: create the API key in the **Navigation menu** > **APIs & Services** > **Credentials**
		```bash
		# start
		# copy the api key and paste here
		export API_KEY=<API_KEY>
		#
		# create request.json
		cat > gcs-request.json <<EOF
		{
		  "config": {
			  "encoding":"FLAC",
			  "languageCode": "en-US"
		  },
		  "audio": {
			  "uri":"gs://cloud-training/gsp323/task4.flac"
		  }
		}
		EOF
		#
		# save the response of the api
		curl -s -X POST -H "Content-Type: application/json" --data-binary @gcs-request.json "https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > task4-gcs.result
		#
		# copy the result file to the bucket
		gsutil cp task4-gcs.result gs://$YOUR_PROJECT-marking/task4-gcs.result
		# end
		
		```
	- Task 2: create the API key in the **Navigation menu** > **APIs & Services** > **Credentials**
		```bash
		# start
		# copy the api key and paste here
		export API_KEY=<API_KEY>
		#
		# run the gcloud to analyse the content
		gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > task4-cnl.result
		#
		# copy the result file to the bucket
		gsutil cp task4-cnl.result gs://$YOUR_PROJECT-marking/task4-cnl.result
		# end
		
		```
	- Task 3: create the API key in the **Navigation menu** > **APIs & Services** > **Credentials** under "Qwiklabs User Account" Section
		```bash
		# start
		# copy the api json file and paste here
		cat > key.json << EOF
		<code_from_json_file>
		EOF
		#
		# create a token
		gcloud auth activate-service-account --key-file key.json
		#
		# export the token
		export TOKEN=$(gcloud auth print-access-token)
		#
		# create request.json
		cat > gvi-request.json <<EOF
		{
		   "inputUri":"gs://spls/gsp154/video/train.mp4",
		   "features": [
			   "LABEL_DETECTION"
		   ]
		}
		EOF
		#
		# save the response of the api
		curl -s -H 'Content-Type: application/json' -H 'Authorization: Bearer '$(gcloud auth print-access-token)'' 'https://videointelligence.googleapis.com/v1/videos:annotate' -d @gvi-request.json > task4-gvi.result
		#
		# copy the result file to the bucket
		gsutil cp task4-gvi.result gs://$YOUR_PROJECT-marking/task4-gvi.result
		# end
		
		```