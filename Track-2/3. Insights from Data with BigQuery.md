## Contents
1.  [Introduction to SQL for BigQuery and Cloud SQL](#Introduction%20to%20SQL%20for%20BigQuery%20and%20Cloud%20SQL)
2. [Challenge Lab](#Challenge%20Lab)
---
####  Introduction to SQL for BigQuery and Cloud SQL
- SELECT : specify what fields you want to pull from your dataset.
- FROM: specify what table or tables we want to pull our data from.
- WHERE: filters tables for specific column values
- GROUP BY: aggregate result-set rows that share common criteria (eg. column value) and will return all of the unique entries found for such criteria
- COUNT(): return the number of rows that share the same criteria (eg. column value)
- AS: creates an _alias_ of a table or column
- ORDER BY: sorts the returned data from a query in ascending or descending order based on a specified criteria or column value (by default: ascending)
- DELETE: __not remove the first row__ of the file per se, but all _rows_ of the table where the column name contains a specified value
- INSERT INTO: requires a table and will create a new row with columns specified by the terms in the first parenthesis. Whatever comes after the "VALUES" clause will be inserted as values in the new row.
- UNION: combines the output of two or more SELECT queries into a result-set.
- HAVING: similar to WHERE except WHERE can't be used on aggregated cols to filter data whereas HAVING clause must be used to filter data on aggregated columns
- WITH: give a sub-query block a name (a process also called sub-query refactoring), which can be referenced in several places within the main SQL query.
	```sql
	-- selecting one col from table
	SELECT USER FROM example_table;

	-- selecting more than one col from table
	SELECT USER, SHIPPED FROM example_table;
	
	-- selecting everything from table
	SELECT * FROM example_table;

	-- selecting only those entries having Shipped entry as YES
	SELECT USER FROM example_table WHERE SHIPPED='YES';

	-- only showing unique start stations
	SELECT start_station_name FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;

	-- counting each uniques station in the dataset
	SELECT start_station_name, COUNT(*) FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;

	-- giving name to the count() col
	SELECT start_station_name, COUNT(*) AS num_starts FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;

	-- sorting the count() col
	-- ascending order
	SELECT start_station_name, COUNT(*) AS num FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name ORDER BY num;
	-- descending order
	SELECT start_station_name, COUNT(*) AS num FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name ORDER BY num DESC;
	
	-- create a database
	-- CREATE DATABASE <name_of_the_database>;
	CREATE DATABASE bike;
	
	-- use the specific database
	-- USE <name_of_the_database>;
	USE bike;
	
	-- create a table in the database
	-- CREATE TABLE <name_of_the_table> (<col_name> <type>, ..);
	CREATE TABLE london1 (start_station_name VARCHAR(255), num INT);
	
	-- delete entries having col entry num as '0'
	DELETE FROM london1 WHERE num=0;
	
	-- insert a new entry as start_testion_name=test destination and num=1
	INSERT INTO london1 (start_station_name, num) VALUES ("test destination", 1);
	
	-- union example by inserting cols from london2 into london1
	SELECT start_station_name AS top_stations, num FROM london1 WHERE num>100000 UNION SELECT end_station_name, num FROM london2 WHERE num>100000 ORDER BY top_stations DESC;
	
	-- eg of HAVING
	SELECT job_title, SUM(salary) department_salary FROM tutorial_db.employee GROUP BY job_title HAVING SUM(salary)>=5000;
	
	-- eg of WITH
	WITH temporaryTable (averageValue) as
    (SELECT avg(Attr1)
    FROM Table)
    SELECT Attr1
    FROM Table, temporaryTable
    WHERE Table.Attr1 > temporaryTable.averageValue;
	```
- SQL Instance
	```bash
	# create the sql instance from gui
	#
	# connect to sql created
	# gcloud sql connect <name_of)the_sql_instance> --user=root
	gcloud sql connect qwiklabs-demo --user=root
	#
	```
- BigQuery
	```bash
	# retrieve the Identity and Access Management (IAM) policy for a table or view and add a binding to the policy,
	# bq add-iam-policy-binding --member=MEMBER_TYPE:MEMBER --role=ROLE [--table] RESOURCE
	# MEMBER_TYPE: user/serviceAccount/group/domain
	bq add-iam-policy-binding --member=user:myAccount@gmail.com --role=roles/bigquery.dataViewer myDataset.myTable
	#
	# bq get-iam-policy command to retrieve the IAM policy for a resource and print it to stdout
	# bq get-iam-policy [FLAGS] RESOURCE
	bq get-iam-policy myDataset.myTable
	#
	# bq remove-iam-policy-binding command to retrieve the IAM policy for a resource and remove a binding from the policy, in one step
	# bq remove-iam-policy-binding FLAGS --member=MEMBER_TYPE:MEMBER --role=ROLE RESOURCE
	#
	# bq set-iam-policy command to specify or update the IAM policy for a resource.
	# bq set-iam-policy [FLAGS] RESOURCE FILE_NAME
	#
	# bq cancel command to cancel BigQuery jobs
	# bq [--synchronous_mode=false] cancel JOB_ID
	# don't want to wait for the bq cancel command to complete ,set the global --synchronous_mode flag to false. The default is `true`
	bq cancel bqjob_12345
	#
	# bq cp command to copy tables, create table snapshots or restore table snapshots
	# bq cp SOURCE_TABLE DESTINATION_TABLE
	bq cp myDataset.myTable myDataset.myTableCopy
	#
	# bq extract command to export table data to Cloud Storage
	# bq extract [--compression=<type>] [--destination_format=<type> [--field_delimiter=<type: 'tab' or '|'>] [--print_header=false] RESOURCE DESTINATION
	bq extract --compression=GZIP --destination_format=CSV --field_delimiter='|' myDataset.myTable gs://myBucket/myFile.csv
	#
	# bq head command to display the specified rows and columns of a table. By default, it displays all columns of the first 100 rows.
	# bq head [--job=JOB] [--max_rows=10] [--start_row=50] [--selected_fields=COLUMN_NAMES] [TABLE]
	bq head --max_rows=10 --start_row=50 --selected_fields=field1,field3 myDataset.myTable
	#
	# bq insert command to insert rows of newline-delimited, JSON-formatted data into a table from a file using the streaming buffer
	# bq insert [--ignore_unknown_values={true|false(default)}] [--skip_invalid_rows={true|false}] [--template_suffix=_insert] [DEST_TABLE] [INPUT_DATA_FILE]
	bq insert --ignore_unknown_values --template_suffix=_insert myDataset.myTable /tmp/myData.json
	#
	# bq load command to load data into a table
	# bq load [FLAGS] DESTINATION_TABLE SOURCE_DATA [SCHEMA]
	bq load myDataset.newTable gs://mybucket/info.csv ./info_schema.json
	#
	# bq ls command to list objects in a collection.
	# bq ls [FLAGS] [RESOURCE]
	bq ls myDataset
	#
	# bq query command to create a query job that runs the specified SQL query
	# bq query [FLAGS] 'QUERY'
	bq query --use_legacy_sql=false 'SELECT word, SUM(word_count) AS count FROM bigquery-public-data.samples.shakespeare WHERE word LIKE "%raisin%" GROUP BY word'
	#
	# bq mk command to create a BigQuery resource 
	# bq mk TYPE_FLAG [OTHER FLAGS] [ARGS]
	bq mk babynames # creates a dataset babynames
	#
	# bq rm command to delete a BigQuery resource
	# bq rm [FLAGS] RESOURCE
	bq rm -r babynames
	#
	# bq show command to display information about a resource
	# bq show [FLAGS] [RESOURCE]
	bq show bigquery-public-data:samples.shakespeare
	#
	# bq update command to change a resource
	# bq update [FLAGS] [RESOURCE]
	
	```

#### Challenge Lab
- Task 1:
	```sql
	SELECT SUM(cumulative_confirmed) AS total_cases_worldwide
	FROM `bigquery-public-data.covid19_open_data.covid19_open_data` 
	WHERE date = "2020-04-15"; 
	```
- Task 2:

	```sql
	WITH 
	    deaths_by_states as 
		(  
			SELECT 
				subregion1_name AS state, 
				sum(cumulative_deceased) AS death_count   
				FROM `bigquery-public-data.covid19_open_data.covid19_open_data` 
				WHERE 
					country_name = "United States of America" 
					AND date = "2020-04-10" 
					AND subregion1_name IS NOT NULL
					GROUP BY subregion1_name
		) 
	SELECT COUNT(*) AS count_of_states 
	FROM deaths_by_states 
	WHERE death_count > 100;
	```
- Task 3:
	```sql
	SELECT * 
	FROM 
	(
		SELECT
		    subregion1_name AS state, 
		    sum(cumulative_confirmed) AS total_confirmed_cases  
	    FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
		WHERE 
		    country_code = "US"
		    AND date = "2020-04-10"
		    AND subregion1_name IS NOT NULL
		GROUP BY subregion1_name 
		ORDER BY total_confirmed_cases DESC 
	) 
	WHERE total_confirmed_cases > 1000;
	```

- Task 4:
	```sql
	SELECT 
	  SUM(cumulative_confirmed) AS total_confirmed_cases,
	  SUM(cumulative_deceased) AS total_deaths,
	  (SUM(cumulative_deceased)/SUM(cumulative_confirmed))*100 AS case_fatality_ratio
	FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
	WHERE 
	   country_name = "Italy" 
	   AND date BETWEEN "2020-04-01" AND "2020-04-30";
	```
- Task 5:
	```sql
	SELECT date
	FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
	WHERE 
	   country_name = "Italy" 
	   AND cumulative_deceased > 10000
	   ORDER BY DATE
	LIMIT 1;
	```
- Task 6:
	```sql
	WITH 
		india_cases_by_date AS 
		(
			SELECT 
				date,
				SUM(cumulative_confirmed) AS cases
			FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
			WHERE
				country_name = "India"
				AND date between '2020-02-21' and '2020-03-15'
		  GROUP BY date
		  ORDER BY date
		), 
		india_previous_day_comparison AS
		(
			SELECT 
				date, 
				cases,
				LAG(cases) OVER(ORDER BY date) AS previous_day,
				cases - LAG(cases) OVER(ORDER BY date) AS net_new_cases
			FROM india_cases_by_date
		)
	SELECT COUNT(date)
	FROM india_previous_day_comparison
	WHERE net_new_cases = 0;
	```
- Task 7:
	```sql
	WITH 
		us_cases_by_date AS 
		(
			SELECT
				date,
				SUM(cumulative_confirmed) AS cases
			FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
			WHERE
				country_name = "United States of America"
				AND date between '2020-03-22' and '2020-04-20'
		  GROUP BY date
		  ORDER BY date
		), 
		us_previous_day_comparison AS
		(
			SELECT
				date,
				cases,
				LAG(cases) OVER(ORDER BY date) AS previous_day,
				cases - LAG(cases) OVER(ORDER BY date) AS net_new_cases,
				(cases - LAG(cases) OVER(ORDER BY date))*100/LAG(cases) OVER(ORDER BY date) AS percentage_increase
			FROM us_cases_by_date
		)
	SELECT
	  date AS Date,
	  cases AS Confirmed_Cases_On_Day,
	  previous_day AS Confirmed_Cases_Previous_Day,
	  percentage_increase AS Percentage_Increase_In_Cases
	FROM us_previous_day_comparison
	WHERE percentage_increase > 10;
	```
- Task 8:
	```sql
	WITH 
		cases_by_country AS 
		(
		  SELECT
			country_name AS country,
			SUM(cumulative_confirmed) AS cases,
			SUM(cumulative_recovered) AS recovered_cases
		  FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
		  WHERE date = "2020-05-10"
		  GROUP BY country_name
		), 
		recovered_rate AS 
		(
		  SELECT 
			country,
			cases,
			recovered_cases,
			(recovered_cases * 100)/cases AS recovery_rate
		  FROM cases_by_country
		)
	SELECT
		country,
		cases AS confirmed_cases,
		recovered_cases,
		recovery_rate
	FROM recovered_rate
	WHERE cases > 50000
	ORDER BY recovery_rate DESC
	LIMIT 10;
	```
- Task 9:
	```sql
	WITH
		france_cases AS 
		(
		  SELECT
			date,
			SUM(cumulative_confirmed) AS total_cases
		  FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
		  WHERE
			country_name="France"
			AND date IN ('2020-01-24', '2020-05-10')
		  GROUP BY date
		  ORDER BY date
		), 
		summary AS 
		(
		  SELECT
		  	total_cases AS first_day_cases,
		    LEAD(total_cases) OVER(ORDER BY date) AS last_day_cases,
		    DATE_DIFF(LEAD(date) OVER(ORDER BY date),date, day) AS days_diff
		  FROM france_cases
		  LIMIT 1
		)
		SELECT 
			first_day_cases, 
			last_day_cases,
			days_diff,
			POWER(last_day_cases/first_day_cases,1/days_diff)-1 AS cdgr
		FROM summary;
	```
- Task 10: Use __DataStudio__ > __BigQuery__ and then in select custom query and paste this query.
	```sql
	SELECT 
		date, 
		SUM(cumulative_confirmed) AS country_cases,
		SUM(cumulative_deceased) AS country_deaths
	FROM `bigquery-public-data.covid19_open_data.covid19_open_data`
	WHERE  
		date BETWEEN "2020-03-15" AND "2020-04-30"
		AND country_name = "United States of America"
	GROUP BY date;
	```

